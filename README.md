# ğŸ§  RAG-Based AI Document Assistant

A powerful AI chatbot that reads, understands, and answers questions from uploaded documents using **RAG (Retrieval-Augmented Generation)**, **FAISS**, **LangChain**, **LLaMA3 (via Ollama)**, and a clean **Streamlit UI**.

---

## ğŸš€ Features

- ğŸ“„ Upload and read documents (PDFs)
- ğŸ¤– Ask questions about the content
- ğŸ’¡ Structured and concise answers
- ğŸ” Chat history and context-aware memory
- â±ï¸ Real-time streaming responses
- ğŸ¨ Styled frontend with dark/light UI enhancements

---

## ğŸ§° Tech Stack

| Layer      | Tech Used                                |
|------------|------------------------------------------|
| Backend    | FastAPI, LangChain, FAISS, Ollama        |
| Embeddings | SentenceTransformers / HuggingFace       |
| LLM        | LLaMA3 via Ollama                        |
| Frontend   | Streamlit                                |

---

## ğŸ“ Project Structure

rag-chatbot/
â”‚
â”œâ”€â”€ rag-backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”‚ â””â”€â”€ routes.py # API endpoints (upload, ask)
â”‚ â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”‚ â”œâ”€â”€ loader.py # Load & split PDFs
â”‚ â”‚ â”‚ â”œâ”€â”€ embedder.py # Embeddings setup
â”‚ â”‚ â”‚ â”œâ”€â”€ vector_store.py # FAISS vector store
â”‚ â”‚ â”‚ â””â”€â”€ qa.py # QA chain setup
â”‚ â”‚ â””â”€â”€ main.py # FastAPI app entrypoint
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ rag-frontend/
â”‚ â”œâ”€â”€ app_ui.py # Streamlit UI
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docs/ # Uploaded PDFs
â”œâ”€â”€ vector_store/ # FAISS index storage
â”œâ”€â”€ README.md # Youâ€™re reading it :)
â””â”€â”€ .gitignore

# ğŸ§‘â€ğŸ’» Getting Started (Local Development)


1. Clone the repo

git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot

2. Setup Python Environment

python -m venv env
Windows: .\env\Scripts\activate

3. Install Dependencies

For Backend:

cd rag-backend
pip install -r requirements.txt

For Frontend:

cd ../rag-frontend
pip install -r requirements.txt

4. Install & Run Ollama (LLM Server)
Install Ollama from https://ollama.com, then run:

ollama run llama3

Let Ollama run in background â€” it powers your LLM.

ğŸš¦ Run the App
1. Start the Backend

cd rag-backend
uvicorn app.main:app --reload

2. Start the Frontend

cd ../rag-frontend
streamlit run app_ui.py

Access app at http://localhost:8501


ğŸ§ª Usage

Upload a PDF in the sidebar.

Ask your question in the chat.

The model retrieves relevant chunks, processes them, and responds.

Chat history is shown with structured formatting.

Use â€œClear Chat Historyâ€ to reset memory.


ğŸŒ Author
Santhosh
Passionate about building smart assistants and intelligent tools.
GitHub: santhosh-xx
