# app/core/qa.py
from langchain.chains import RetrievalQA
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate

def create_qa_chain(vector_store, memory=None, callbacks=None):
    llm = OllamaLLM(model="llama3", temperature=0.2, callbacks=callbacks)

    prompt = PromptTemplate.from_template(
    """
    You are a highly intelligent expert assistant trained to answer complex questions based on the given context.
    Use only the provided context to construct your response. If the answer is not clearly present or cannot be inferred, respond with: "I don't know."

    Your goal is to provide **concise, structured, and helpful answers** in a readable format.

    ---üß† INSTRUCTIONS---
    1. **Start** with a clear, one-line summary of the answer.
    2. **Follow** with bullet points for key facts, insights, or definitions.
    3. **Use** numbered steps for procedures, workflows, or processes.
    4. **Include** code snippets or formulas only if relevant.
    5. **Avoid** hallucinating or making up information not in the context.
    6. **Maintain** a calm, professional, and accurate tone.

    ---üìö CONTEXT---
    {context}

    ---‚ùì QUESTION---
    {question}

    ---‚úÖ STRUCTURED ANSWER FORMAT---
    [Summary line]
    - Point 1
    - Point 2
    - ...

    1. Step-by-step (if applicable)
    2. ...

    (Add code snippets, quotes, or markdown explanations only if needed)

    Let's begin:
    """
)

    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vector_store.as_retriever(),
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True,
        memory=memory,
        output_key="result",
        verbose=True
    )
